epochs : 4000
steps_per_epoch : 4000 # Max number of (s,a) per epoch
max_ep_len : 25 # Max len of a trajectory / episode / rollout
v_train_iters : 80 # Number of iterations to run the value function optimizer in each epoch
obs_reward : -0.1
dirt_reward : 2.0
ene_reward : -0.1


gamma : 0.99 # Discount factor
lambda : 0.97 # Hyperparameter for the baseline
pi_lr : 3e-4 # Learning rate for the training of the policy net
v_lr : 1e-3 # Learning rate for the training of the value function net
hidden_sizes : [64,32] # Shape of each FC hidden layer

window_size : 5
save_dir : /home/tmr/Documents/PhD/My_PhD/WASP-courses/WASP/6_AI_ML/assignment/wasp-AI-ML-m1/outputs/
save_plot : True
show_plot: False